---
toc: true
layout: post
description: Summary and Key points in the paper- *Differentiation of Blackbox combinatorial solvers*.
categories: [paper]
title: Differentiation of Blackbox combinatorial solvers
---

## L1: Simple problem-solution description: 

Fundamentally, the paper aims to tackle combining gradient-based optimizeatin of neura-networks when black-box solvers are present by treating them as an intermediate layer. At the core, the misconception of differentiability is discussed, where the paper states the gradient recevied by traditional differentiation approaches leads to zero gradient since inputs to the black-box solvers are discrete in nature, and meanigless to the neural networks. Hence, they construct a *continuous interpolation* of the discrete points so that the gradient passed along is meaningful.

## L2: Key Details

1. The linear interpolation between the discrete point - *continuous interpolation* is used to calculate the gradient, with an additional hyperparameter $\lambda$ that controls the tradeoff between the "gradient" and "faithfulness to the original function".
2. This can be extended to **any** blackbox solvers and potentially has huge impact on various problems that have complex constraints.


## L3: Coming soon(proofs, detailed analysis and future work)

Slightly secretive since it's part of on-going work. Will be up soon.
