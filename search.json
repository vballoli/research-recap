[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/concept-models/index.html",
    "href": "posts/concept-models/index.html",
    "title": "Concept Bottleneck Models",
    "section": "",
    "text": "Imagine deploying a neural network. That’s it, that’s the joke.\nNow really imagine deploying a neural network or any other machine learning model that outputs some prediction on the given input. For example, let’s say it is a dog classifier i.e. given an image of a dog, it would output what breed of dog it could be. Now what if the model here first classified if the dog has certain visual features like is_furry is set to true, is_golden is set to true, has_white_hair is set to false. Only after predicting these features does the model then classify the dog to be a golden retriever. So, why is this important? After all, does it matter if these predictions are latent or shown? The key feature in this exercise is this - let’s say the the image was grainy and the network predicted is_furry feature to be false - you could now correct it and set it to true because it’s pretty easy for us humans to do so even without knowing what a golden retriever is. The model will now most likely classify the image as golden_retriever, thanks to your correction!\nThis is the promise of Concept Models - the ability for humans to intervene in the prediction process by being allowed to modify the presence or absence of concepts in the input - images, text, etc. This idea was first formalized in Concept Bottleneck Models and numerous works have followed since that build up on this idea and really try to make it compatible to humans!"
  },
  {
    "objectID": "posts/concept-models/index.html#preparing-your-data-what-goes-into-training-a-traditional-cbm",
    "href": "posts/concept-models/index.html#preparing-your-data-what-goes-into-training-a-traditional-cbm",
    "title": "Concept Bottleneck Models",
    "section": "Preparing your data: what goes into training a traditional CBM",
    "text": "Preparing your data: what goes into training a traditional CBM\n\nPre-defined list concepts. Let’s take the example of the CUB dataset. Some concepts in this dataset include black undertail color, yellow throat color, etc.\nAnnotating these pre-defined list of concepts and the associated class label with it.\nThat’s it!"
  },
  {
    "objectID": "posts/concept-models/index.html#concepts-you-need-to-build-your-own-cbm",
    "href": "posts/concept-models/index.html#concepts-you-need-to-build-your-own-cbm",
    "title": "Concept Bottleneck Models",
    "section": "Concepts you need to build your own CBM",
    "text": "Concepts you need to build your own CBM\nYes, very much intended. Moving on, here are the concepts you need to build your own CBM:\n\nConcept Predictor Model: depending on your modality, it can be pre-trained encoders, pre-trained image classifiers, etc. Pre-training does help a lot as evidenced by tons of literature in the deep learning community over the past 7 years maybe. The key part is here the activation function used to predict the concepts: while sigmoid might seem like the obvious choice, sigmoid is shown to squash the activations a lot and you end up with not so great performance. ReLUs on the other hand perform well but their intervention needs a special computation. The intuition here is if you were to intervene in sigmoid, you’d either set the concept close to zero if it is absent or close to one if it is present, which is not the case for ReLUs simply because the maximum activation you could achieve here is pretty high. Therefore, the authors suggest computing the 95th and 5th percentile of the activations for each of the concept to indicate presence and absence respectively and set the intervention values the same when deploying.\nA simple neural network or actually any machine learning model you fancy that takes in these inputs and predicts the final label or value depending on classification or regression."
  },
  {
    "objectID": "posts/dev-setup/index.html",
    "href": "posts/dev-setup/index.html",
    "title": "Development Setup",
    "section": "",
    "text": "My programming language of choice has mostly been Python lately, with dev both on Linux and Windows (and WSL) - both when I’m working on personal and professional projects. While some of it keeps me awake for quite a long time based on the criticality of the project, it becomes essential that the tools and commands I use are handy, ubiquitous and uniform across my machines. This post showcases some of these and will be updated continuously."
  },
  {
    "objectID": "posts/dev-setup/index.html#libraries-and-frameworks",
    "href": "posts/dev-setup/index.html#libraries-and-frameworks",
    "title": "Development Setup",
    "section": "Libraries and Frameworks",
    "text": "Libraries and Frameworks\n\nNumba - makes things go vroom.\nFastcore - reduces regular boilerplate by quite a lot.\nStreamlit - host dashboard to present to multiple people asynchronously and remotely.\nRay - large scale distributed code.\nDask - host cluster on remote machines and send compute to these clusters. fast pandas alternative\nSphinx - build and host documentation\nFastAPI - web framework of choice\nPostgreSQL - database of choice\nsqlalchemy - ORM of choice\nHydra - config management\nWandb/mlflow - experiment tracker personal/professional"
  },
  {
    "objectID": "posts/research-code/index.html",
    "href": "posts/research-code/index.html",
    "title": "Lessons from writing Research Code",
    "section": "",
    "text": "Having written a good amount of research code for a while now, I was wondering what “good research code” can be construed as. This topic has been discussed heavily with a lot of good blog posts and materials on the internet (some I’ve found: Hongyuan Mei’s blog, Good Research Code Handbook, etc.). Personally, I have a checklist of things that potrays my version of the basic requirements of good research code, in the context of ease of use for other researchers/practioners:\nSpecial note: A good documentation in my observation(see point 2. above) has immensely helped me in navigating relatively new fields of research, particularly in Machine Learning."
  },
  {
    "objectID": "posts/research-code/index.html#but-why-invest-additional-time-in-this",
    "href": "posts/research-code/index.html#but-why-invest-additional-time-in-this",
    "title": "Lessons from writing Research Code",
    "section": "But why invest additional time in this ?",
    "text": "But why invest additional time in this ?\nThe recent trend suggests that most research code goes through multiple phases:\n\nQuick, dirty implementation of the idea in mind\nWrite scripts to get results\nMake code public\n\nIdeally, it should be something like this:\n\nQuick, dirty implementation of the idea in mind.\nTidy code, structured implementation of both the boiler plate, previous work and the core contribution.\nWrite reproducible scripts and automate most metric collection, results charting and documentation to reduce manual work.\nOnce code goes public, spending some time in maintaining/addressing user issues.\n\nYou’ll notice it’s the second point that differentiates between good and terrible practices - tidying up code at the right time. Given the results of the quick prototype that backed your idea, it’s clear that you’ll be investing a good portion of your time diving deep into all the nuances of this prototype. This means there should be more research and less manual work. What manual work refers to here is often times bad code can result in a lot of manual work of coarsing through csvs, logs, metadata, and other things that shifts the focus from research to running scripts manually.\nThis means the tools you use(IDE, software, hardware) etc. is something you should be familiar with, mostly from previous experience on other projects. This ensures that: 1. Not a lot of time is spent on setup (at the start of the project) and publishing (at the end of the project) 2. Working within a comfortable zone of tools ensures productivity and consistency.\nMy dev setup is a culmination of mostly research and some non-research tools that is mostly oriented towards working on Windows based local machines with WSL and Linux machines with GPUs."
  },
  {
    "objectID": "posts/offline-rl/index.html",
    "href": "posts/offline-rl/index.html",
    "title": "Offline Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning(RL), as simply stated in Sutton and Barto Sutton and Barto (2018) , is learning what to do i.e map situations to actions to maximize the cumulative reward for a series of steps. Naturally, trial and error is at the core of Reinforcement Learning i.e. RL learns in a hands-on approach exploring and exploiting its task, which is referred to as online learning, where online refers to agent manipulating the surroundings around it(commonly referred to as an Environment) with the available controls it has(referred to as actions). While such approaches have been popular and successful in many tasks, they often rely on continually improving the ability to search better and learn on the go, which does not translate well to tasks where the ability to explore the environment comes with a cost, which harms the ability of most state-of-the-art algorithms to learn. Hence, Offline Reinforcement Learning is a paradigm within RL that is tasked to learn optimal behaviour from a static dataset i.e. the available data is fixed and cannot be controlled, as it is often generated from an external agent. The figure below illustrates the key difference between traditional RL and offline RL.\n\n\n\nIllustration of Traditional RL interactions vs Offline RL(credit: Offline RL NeurIPS )\n\n\nIn this post, I cover some popular algorithms and code excerpts that help understand some of the implementation details regarding these algorithms to provide the intuition of how they work. For more documentation on how to run these algorithms, you can check out my repository that implements these algorithms in detail at Offlax:\n\n\nConservative Q-Learning(CQL) builds on top of existing algorithms like QR-DQN(for discrete action spaces) and SAC(for continuous action spaces). As the paper suggests, CQL only requires 20 lines of additional code over the existing RL algorithms like QR-DQN and SAC.\n\n\n\nBehavior cloning is a widely used techniques to for agents to learn from an expert i.e. trying to understand based on an expert agent’s trajectory. This paper suggests using Behavior Cloning with some adjustments over an existing RL algorithm like TD3 to learn in from a static dataset of trajectories."
  },
  {
    "objectID": "posts/offline-rl/index.html#conservative-q-learning-kumar2020conservative",
    "href": "posts/offline-rl/index.html#conservative-q-learning-kumar2020conservative",
    "title": "Offline Reinforcement Learning",
    "section": "",
    "text": "Conservative Q-Learning(CQL) builds on top of existing algorithms like QR-DQN(for discrete action spaces) and SAC(for continuous action spaces). As the paper suggests, CQL only requires 20 lines of additional code over the existing RL algorithms like QR-DQN and SAC."
  },
  {
    "objectID": "posts/offline-rl/index.html#td3-behavior-cloning-fujimoto2021minimalist",
    "href": "posts/offline-rl/index.html#td3-behavior-cloning-fujimoto2021minimalist",
    "title": "Offline Reinforcement Learning",
    "section": "",
    "text": "Behavior cloning is a widely used techniques to for agents to learn from an expert i.e. trying to understand based on an expert agent’s trajectory. This paper suggests using Behavior Cloning with some adjustments over an existing RL algorithm like TD3 to learn in from a static dataset of trajectories."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Recap",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nConcept Bottleneck Models\n\n\n\n\n\n\nhuman-AI collaboration\n\n\n\n\n\n\n\n\n\nJan 6, 2024\n\n\nVaibhav Balloli\n\n\n\n\n\n\n\n\n\n\n\n\nOffline Reinforcement Learning\n\n\n\n\n\n\nRL\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\nVaibhav Balloli\n\n\n\n\n\n\n\n\n\n\n\n\nLessons from writing Research Code\n\n\n\n\n\n\ncode\n\n\nsetup\n\n\nresearch\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nVaibhav Balloli\n\n\n\n\n\n\n\n\n\n\n\n\nDevelopment Setup\n\n\n\n\n\n\ncode\n\n\nsetup\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\nVaibhav Balloli\n\n\n\n\n\n\nNo matching items"
  }
]