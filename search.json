[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Research Recap",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nFolded Optimization for End-to-End Model-Based Learning: A summary\n\n\n\n\n\n\n\nOptimization\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2023\n\n\nVaibhav Balloli\n\n\n\n\n\n\n\n\nOffline Reinforcement Learning\n\n\n\n\n\n\n\nRL\n\n\n\n\n\n\n\n\n\n\n\nJan 28, 2023\n\n\nVaibhav Balloli\n\n\n\n\n\n\n\n\nLessons from writing Research Code\n\n\n\n\n\n\n\ncode\n\n\nsetup\n\n\nresearch\n\n\n\n\n\n\n\n\n\n\n\nAug 15, 2022\n\n\nVaibhav Balloli\n\n\n\n\n\n\n\n\nDevelopment Setup\n\n\n\n\n\n\n\ncode\n\n\nsetup\n\n\n\n\n\n\n\n\n\n\n\nJul 12, 2022\n\n\nVaibhav Balloli\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/dev-setup/index.html",
    "href": "posts/dev-setup/index.html",
    "title": "Development Setup",
    "section": "",
    "text": "My programming language of choice has mostly been Python lately, with dev both on Linux and Windows (and WSL) - both when I’m working on personal and professional projects. While some of it keeps me awake for quite a long time based on the criticality of the project, it becomes essential that the tools and commands I use are handy, ubiquitous and uniform across my machines. This post showcases some of these and will be updated continuously."
  },
  {
    "objectID": "posts/dev-setup/index.html#libraries-and-frameworks",
    "href": "posts/dev-setup/index.html#libraries-and-frameworks",
    "title": "Development Setup",
    "section": "Libraries and Frameworks",
    "text": "Libraries and Frameworks\n\nNumba - makes things go vroom.\nFastcore - reduces regular boilerplate by quite a lot.\nStreamlit - host dashboard to present to multiple people asynchronously and remotely.\nRay - large scale distributed code.\nDask - host cluster on remote machines and send compute to these clusters. fast pandas alternative\nSphinx - build and host documentation\nFastAPI - web framework of choice\nPostgreSQL - database of choice\nsqlalchemy - ORM of choice\nHydra - config management\nWandb/mlflow - experiment tracker personal/professional"
  },
  {
    "objectID": "posts/fold-opt/index.html",
    "href": "posts/fold-opt/index.html",
    "title": "Folded Optimization for End-to-End Model-Based Learning: A summary",
    "section": "",
    "text": "Folded Optimization for End-to-End Model-Based Learning by Kotary, Dinh, and Fioretto (2023) is an exciting new paper that tackles the problem of integrating constrained optimization models/solvers into the neural network pipeline. For example, a scenario where the outputs of a neural network act as inputs to these solvers that output the optimal decisions. This problem has been tackled before in various forms depending on the target task and the assumptions made to relax some of the differentiability constraints(see Donti, Rolnick, and Kolter (2021), Ferber et al. (2020), Pogančić et al. (2020) and many more). More recently, this line of work are being turned into libraries(Amos and Kolter (2017), Pineda et al. (2022)) built on top of frameworks capable of autograd like PyTorch for users to model their own scenarios without having to implement them from scratch. This is an important problem, especially when there a lot of tasks in various industries and applications that require decision making under non-convex constraints and uncertain data.\nIn this post, I summarize the paper by covering some preliminary concepts, present the intuition behind the contribution and go over some key takeaways from this. This paper also provides a library fold-opt that is yet to be open sourced, so I’ll also be covering that once that happens."
  },
  {
    "objectID": "posts/fold-opt/index.html#unfolding-and-unrolling",
    "href": "posts/fold-opt/index.html#unfolding-and-unrolling",
    "title": "Folded Optimization for End-to-End Model-Based Learning: A summary",
    "section": "Unfolding and Unrolling",
    "text": "Unfolding and Unrolling\nLet’s uncover unfolding and unrolling. Unrolling is a simple process of backpropogating the optimization through the computation graph without explicity calculating their jacobians."
  },
  {
    "objectID": "posts/offline-rl/index.html",
    "href": "posts/offline-rl/index.html",
    "title": "Offline Reinforcement Learning",
    "section": "",
    "text": "Reinforcement Learning(RL), as simply stated in Sutton and Barto Sutton and Barto (2018) , is learning what to do i.e map situations to actions to maximize the cumulative reward for a series of steps. Naturally, trial and error is at the core of Reinforcement Learning i.e. RL learns in a hands-on approach exploring and exploiting its task, which is referred to as online learning, where online refers to agent manipulating the surroundings around it(commonly referred to as an Environment) with the available controls it has(referred to as actions). While such approaches have been popular and successful in many tasks, they often rely on continually improving the ability to search better and learn on the go, which does not translate well to tasks where the ability to explore the environment comes with a cost, which harms the ability of most state-of-the-art algorithms to learn. Hence, Offline Reinforcement Learning is a paradigm within RL that is tasked to learn optimal behaviour from a static dataset i.e. the available data is fixed and cannot be controlled, as it is often generated from an external agent. The figure below illustrates the key difference between traditional RL and offline RL.\n\n\n\nIllustration of Traditional RL interactions vs Offline RL(credit: Offline RL NeurIPS )\n\n\nIn this post, I cover some popular algorithms and code excerpts that help understand some of the implementation details regarding these algorithms to provide the intuition of how they work. For more documentation on how to run these algorithms, you can check out my repository that implements these algorithms in detail at Offlax:\n\n\nConservative Q-Learning(CQL) builds on top of existing algorithms like QR-DQN(for discrete action spaces) and SAC(for continuous action spaces). As the paper suggests, CQL only requires 20 lines of additional code over the existing RL algorithms like QR-DQN and SAC.\n\n\n\nBehavior cloning is a widely used techniques to for agents to learn from an expert i.e. trying to understand based on an expert agent’s trajectory. This paper suggests using Behavior Cloning with some adjustments over an existing RL algorithm like TD3 to learn in from a static dataset of trajectories."
  },
  {
    "objectID": "posts/research-code/index.html",
    "href": "posts/research-code/index.html",
    "title": "Lessons from writing Research Code",
    "section": "",
    "text": "Having written a good amount of research code for a while now, I was wondering what “good research code” can be construed as. This topic has been discussed heavily with a lot of good blog posts and materials on the internet (some I’ve found: Hongyuan Mei’s blog, Good Research Code Handbook, etc.). Personally, I have a checklist of things that potrays my version of the basic requirements of good research code, in the context of ease of use for other researchers/practioners:\nSpecial note: A good documentation in my observation(see point 2. above) has immensely helped me in navigating relatively new fields of research, particularly in Machine Learning."
  },
  {
    "objectID": "posts/research-code/index.html#but-why-invest-additional-time-in-this",
    "href": "posts/research-code/index.html#but-why-invest-additional-time-in-this",
    "title": "Lessons from writing Research Code",
    "section": "But why invest additional time in this ?",
    "text": "But why invest additional time in this ?\nThe recent trend suggests that most research code goes through multiple phases:\n\nQuick, dirty implementation of the idea in mind\nWrite scripts to get results\nMake code public\n\nIdeally, it should be something like this:\n\nQuick, dirty implementation of the idea in mind.\nTidy code, structured implementation of both the boiler plate, previous work and the core contribution.\nWrite reproducible scripts and automate most metric collection, results charting and documentation to reduce manual work.\nOnce code goes public, spending some time in maintaining/addressing user issues.\n\nYou’ll notice it’s the second point that differentiates between good and terrible practices - tidying up code at the right time. Given the results of the quick prototype that backed your idea, it’s clear that you’ll be investing a good portion of your time diving deep into all the nuances of this prototype. This means there should be more research and less manual work. What manual work refers to here is often times bad code can result in a lot of manual work of coarsing through csvs, logs, metadata, and other things that shifts the focus from research to running scripts manually.\nThis means the tools you use(IDE, software, hardware) etc. is something you should be familiar with, mostly from previous experience on other projects. This ensures that: 1. Not a lot of time is spent on setup (at the start of the project) and publishing (at the end of the project) 2. Working within a comfortable zone of tools ensures productivity and consistency.\nMy dev setup is a culmination of mostly research and some non-research tools that is mostly oriented towards working on Windows based local machines with WSL and Linux machines with GPUs."
  }
]